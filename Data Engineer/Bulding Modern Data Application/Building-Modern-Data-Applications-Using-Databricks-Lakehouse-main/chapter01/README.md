## Chapter 1 - Introduction to Delta Live Tables

In this chapter, we're going to dive straight into creating and running your very first Delta Live Tables (DLT) pipeline.

To follow along in this chapter, it's recommended to have Databricks workspace permissions to create an all-purpose cluster and a DLT pipeline using a cluster policy.

Furthermore, you will need to download and execute the accompanying notebook samples:

- `My First DLT Pipeline.py` -  An introduction to the DLT framework using a basic example for declaring a target dataset using the framework.

### Technical requirements
To follow along in this chapter, you will need to have Databricks workspace permissions to create and start an all-purpose cluster so that you can execute all of the accompanying notebook cells. You will also need permissions to create and run a new DLT pipeline using a cluster policy. It's recommended to have Unity Catalog permissions to create and use Catalogs, Schemas, and Tables.

### Expected costs
This chapter will create and run several new notebooks and Delta Live Table pipelines using the `Core` product edition. As a result, the pipelines are estimated to consume around 5-10 Databricks Units (DBUs).

Please see the Databricks documentation for the latest pricing figures: [Pricing calculator](https://www.databricks.com/product/pricing/product-pricing/instance-types).
