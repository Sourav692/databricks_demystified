# Databricks notebook source
import dlt
import pyspark.sql.functions as F

# COMMAND ----------

# The schema location directory keeps track of your data schema over time
SCHEMA_LOCATION = "/tmp/chp_10/iot_device_data_chkpnt"

# The location where raw trip data is written
RAW_DATA_LOCATION = "/tmp/chp_10/iot_device_data/"

# COMMAND ----------

@dlt.table(
  name="raw_iot_device_data",
  comment="Raw IoT device data generated by the data generator notebook"
)
def raw_iot_device_data():
  return (
  spark.readStream.format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", SCHEMA_LOCATION)
  .load(RAW_DATA_LOCATION)
)

# COMMAND ----------

@dlt.table(name="iot_device_data_silver",
           comment="IoT device data with transformed columns")
def iot_device_data_silver():
    return (
        dlt.read("raw_iot_device_data")
        .withColumn("humidity_rounded", F.round(F.col("humidity").cast("float"), 2))
        .withColumn("temperature_rounded", F.round(F.col("temperature").cast("float"), 2))
        .drop("humidity", "temperature")
    )
