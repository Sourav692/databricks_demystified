{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef23ea6-7dda-496e-8f4a-11242edd61da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Demo Data"
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # CREATE DEMO SOURCE DELTA TABLE\n",
    "# # Run this in a standard notebook before creating the DLT pipeline\n",
    "# # ===================================================================\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import *\n",
    "\n",
    "# print(\"Creating demo source table...\")\n",
    "\n",
    "# # Create realistic but simple sales data\n",
    "# initial_sales_data = [\n",
    "#     Row(order_id=1, customer_id=101, product=\"Laptop\", amount=1200.0, order_date=\"2024-01-01\"),\n",
    "#     Row(order_id=2, customer_id=102, product=\"Mouse\", amount=25.0, order_date=\"2024-01-02\"),\n",
    "#     Row(order_id=3, customer_id=103, product=\"Keyboard\", amount=75.0, order_date=\"2024-01-03\"),\n",
    "#     Row(order_id=4, customer_id=101, product=\"Monitor\", amount=300.0, order_date=\"2024-01-04\"),\n",
    "# ]\n",
    "\n",
    "# # Create DataFrame and write as Delta table\n",
    "# sales_df = spark.createDataFrame(initial_sales_data)\n",
    "# sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_university.dlt.demo_sales_01\")\n",
    "\n",
    "# print(\"Demo source table created with 4 records:\")\n",
    "# spark.sql(\"SELECT * FROM data_university.dlt.demo_sales_01\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2993649a-dec2-4fb5-8697-f7f07cc535e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming Table vs. Materialized View"
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DLT PIPELINE: STREAMING TABLE vs MATERIALIZED VIEW DEMO\n",
    "# Use this notebook as source code for your DLT pipeline\n",
    "# ===================================================================\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import col, sum as pysum, count, desc, avg\n",
    "\n",
    "# Pipeline parameters for source configuration\n",
    "catalog = \"data_university\"\n",
    "schema = \"dlt\"\n",
    "sales_source_table = f\"{catalog}.{schema}.demo_sales_01\"\n",
    "\n",
    "print(f\"Reading from source: {sales_source_table}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STREAMING TABLE: Uses spark.readStream for incremental processing\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_streaming_sales\",\n",
    "    comment=f\"DLT Streaming Table - Incremental processing from {sales_source_table}\"\n",
    ")\n",
    "def bronze_streaming_sales():\n",
    "    \"\"\"\n",
    "    This creates a STREAMING TABLE in DLT.\n",
    "    - Uses spark.readStream for incremental data processing\n",
    "    - Only processes new/changed records since last run\n",
    "    - Maintains checkpoints for exactly-once processing\n",
    "    - Ideal for Bronze layer and real-time ingestion\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(sales_source_table)\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# MATERIALIZED VIEW: Uses spark.read for batch processing\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_materialized_sales\",\n",
    "    comment=f\"DLT Materialized View - Batch processing from {sales_source_table}\"\n",
    ")\n",
    "def bronze_materialized_sales():\n",
    "    \"\"\"\n",
    "    This creates a MATERIALIZED VIEW in DLT.\n",
    "    - Uses spark.read for batch data processing\n",
    "    - Reprocesses ALL data on each pipeline run\n",
    "    - No incremental checkpointing\n",
    "    - Ideal for Silver/Gold layers and analytics\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .table(sales_source_table)\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# SILVER LAYER: Compare downstream processing from both sources\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_customer_summary_streaming\",\n",
    "    comment=\"Customer summary derived from streaming table\"\n",
    ")\n",
    "def silver_customer_summary_streaming():\n",
    "    \"\"\"\n",
    "    This processes data from the streaming table.\n",
    "    Updates will be incremental based on new data.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.bronze_streaming_sales\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            pysum(\"amount\").alias(\"total_spent\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_spent\"))\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_customer_summary_materialized\",\n",
    "    comment=\"Customer summary derived from materialized view\"\n",
    ")\n",
    "def silver_customer_summary_materialized():\n",
    "    \"\"\"\n",
    "    This processes data from the materialized view.\n",
    "    Updates will recompute all data on each run.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.bronze_materialized_sales\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            pysum(\"amount\").alias(\"total_spent\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_spent\"))\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# GOLD LAYER: Product performance analysis\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_product_performance\",\n",
    "    comment=\"Product performance metrics from streaming data\"\n",
    ")\n",
    "def gold_product_performance():\n",
    "    \"\"\"\n",
    "    Product analysis showing the final aggregated results.\n",
    "    This demonstrates how streaming data flows to analytics.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.bronze_streaming_sales\")\n",
    "        .groupBy(\"product\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"order_count\"),\n",
    "            pysum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(desc(\"total_revenue\"))\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8612f9-9737-43cc-a2b2-34fbc1919261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# VIEW: Non-materialized intermediate processing\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"high_value_orders\",\n",
    "    comment=\"View of orders above $100 - demonstrates DLT views\"\n",
    ")\n",
    "def high_value_orders():\n",
    "    \"\"\"\n",
    "    DLT View (not materialized) for high-value orders.\n",
    "    This shows the difference between views and tables in DLT.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.bronze_streaming_sales\")\n",
    "        .filter(col(\"amount\") > 100.00)\n",
    "        .select(\"order_id\", \"customer_id\", \"product\", \"amount\", \"order_date\")\n",
    "        .orderBy(desc(\"amount\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d88e922-b69b-4f32-98f6-947658b7fb17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add New Data in Source Table"
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # SIMULATE NEW DATA ARRIVAL\n",
    "# # Run this after the first DLT pipeline execution\n",
    "# # ===================================================================\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "\n",
    "# print(\"Adding new sales data to simulate incremental ingestion...\")\n",
    "\n",
    "# # Add new sales records\n",
    "# new_sales_data = [\n",
    "#     Row(order_id=5, customer_id=104, product=\"Tablet\", amount=450.0, order_date=\"2024-01-05\"),\n",
    "#     Row(order_id=6, customer_id=102, product=\"Headphones\", amount=150.0, order_date=\"2024-01-06\"),\n",
    "#     Row(order_id=7, customer_id=101, product=\"Webcam\", amount=80.0, order_date=\"2024-01-07\"),\n",
    "# ]\n",
    "\n",
    "# new_sales_df = spark.createDataFrame(new_sales_data)\n",
    "# new_sales_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.demo_sales_01\")\n",
    "\n",
    "# print(\"New data added! Source table now contains:\")\n",
    "# spark.sql(\"SELECT COUNT(*) as total_records FROM data_university.dlt.demo_sales_01\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a370ac-cde6-42be-aa52-736e5391c163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify both tables have the same final data\n",
    "-- SELECT 'Streaming Table' as table_type, COUNT(*) as record_count \n",
    "-- FROM data_university.dlt.bronze_streaming_sales\n",
    "-- UNION ALL\n",
    "-- SELECT 'Materialized View' as table_type, COUNT(*) as record_count \n",
    "-- FROM data_university.dlt.bronze_materialized_sales;\n",
    "\n",
    "-- Compare customer summaries (should be identical)\n",
    "-- SELECT 'From Streaming' as source, * FROM data_university.dlt.silver_customer_summary_streaming\n",
    "-- UNION ALL  \n",
    "-- SELECT 'From Materialized' as source, * FROM data_university.dlt.silver_customer_summary_materialized\n",
    "-- ORDER BY source, total_spent DESC;\n",
    "\n",
    "-- Check the high-value orders view\n",
    "-- SELECT * FROM data_university.dlt.high_value_orders;\n",
    "\n",
    "-- View product performance\n",
    "-- SELECT * FROM data_university.dlt.gold_product_performance;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fdc56b-9cc0-45f8-a8f4-1278d2b11dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8637459336718476,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Difference of MV and Streaming Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
