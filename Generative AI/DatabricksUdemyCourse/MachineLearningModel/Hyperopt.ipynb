{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5549ede-62fe-4bf8-93c1-9c9c27244253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a489cf05-62df-4c5b-b27f-b1f03832fb61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading the CSV dataset in the DBFS (Databricks File System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9deac0c8-0dfd-45f1-a156-f5bc27397f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/diabetes.csv https://raw.githubusercontent.com/kuljotSB/DatabricksUdemyCourse/refs/heads/main/MachineLearningModel/diabetes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a122733b-ce0d-4665-acde-87d6ebe569df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Splitting the dataset into train and test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6ba471-7b92-49c3-8e28-3249901542ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mlflow_lab/diabetes.csv\")\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ae5889-b679-40ba-bbbc-33c7c8d64198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimizing Hyperparameter values for our ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b25f0764-1aad-4ba4-b4fb-08572a514f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "import mlflow\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "   \n",
    "def objective(params):\n",
    "    # Train a model using the provided hyperparameter value\n",
    "    numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    mlAlgo = DecisionTreeClassifier(labelCol=\"Outcome\",    \n",
    "                                    featuresCol=\"Features\",\n",
    "                                    maxDepth=params['MaxDepth'], maxBins=params['MaxBins'])\n",
    "    pipeline = Pipeline(stages=[numVector, numScaler, featureVector, mlAlgo])\n",
    "    model = pipeline.fit(train)\n",
    "       \n",
    "    # Evaluate the model to get the target metric\n",
    "    prediction = model.transform(test)\n",
    "    eval = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = eval.evaluate(prediction)\n",
    "       \n",
    "    # Hyperopt tries to minimize the objective function, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b78a3d2-d84f-4059-a330-130a8744e04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Defining the Search Space for our hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06901a03-0c7f-43f7-8543-d93180a8359f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Also will be logging each hyperparameter run using a Trials() object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ba9e9e-9ff9-4fa2-9c3c-88378918ad66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "from hyperopt import fmin, tpe, hp\n",
    "  \n",
    "search_space = {\n",
    "    'MaxDepth': hp.randint('MaxDepth', 10),\n",
    "    'MaxBins': hp.choice('MaxBins', [10, 20, 30])\n",
    "}\n",
    "\n",
    "algo=tpe.suggest\n",
    "\n",
    "# Create a Trials object to track each run\n",
    "trial_runs = Trials()\n",
    "   \n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=3,\n",
    "  trials=trial_runs)\n",
    "   \n",
    "print(\"Best param values: \", argmin)\n",
    "   \n",
    "# Get details from each trial run\n",
    "print (\"trials:\")\n",
    "for trial in trial_runs.trials:\n",
    "    print (\"\\n\", trial)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Hyperopt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
