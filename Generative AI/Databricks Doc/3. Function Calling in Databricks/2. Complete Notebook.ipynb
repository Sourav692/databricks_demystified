{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba891f11-1660-4fa0-a0f5-55f66f3cb5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Function calling using Foundation Model APIs\n",
    "\n",
    "This notebook demonstrates how the *function calling* (or *tool use*) API can be used to extract structured information from natural language inputs using the large language models (LLMs) made available using Foundation Model APIs. This notebook uses the OpenAI SDK to demonstrate interoperability.\n",
    "\n",
    "\n",
    "LLMs generate output in natural language, the exact structure of which is hard to predict even when the LLM is given precise instructions. Function calling forces the LLM to adhere to a strict schema, making it easy to automatically parse the LLM's outputs. This unlocks advanced use cases, enabling LLMs to be components in complex data processing pipelines and Agent workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147a369f-f071-42f0-af90-1a8ef7d6c443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d3ad22ca-d7ab-4c63-bc2b-b9e2ed0c19b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install libraries used in this demo"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai tenacity tqdm\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e335035c-6398-4356-81b3-bffd7966f722",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select model endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# The endpoint ID of the model to use. Not all endpoints support function calling.\n",
    "MODEL_ENDPOINT_ID = \"databricks-meta-llama-3-3-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1749b5fc-4b86-44e7-9e67-0d9a7f622cd2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up API client"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from openai import OpenAI, RateLimitError\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception,\n",
    ")  # for exponential backoff\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# A token and the workspace's base FMAPI URL are needed to talk to endpoints\n",
    "fmapi_token = \"\"\n",
    "fmapi_base_url = (\n",
    "    f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}/serving-endpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d1814dd-a697-4972-bfaf-4f6fae2821c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The following defines helper functions that assist the LLM to respond according to the specified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06ed657-d175-4ffa-907e-78e1ad7bdb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=fmapi_token, base_url=fmapi_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ad266e-5640-4332-a354-591a264d5d15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up helper functions"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# NOTE: We *strongly* recommend handling retry errors with backoffs, so your code gracefully degrades when it bumps up against pay-per-token rate limits.\n",
    "@retry(\n",
    "    wait=wait_random_exponential(min=1, max=30),\n",
    "    stop=stop_after_attempt(3),\n",
    "    retry=retry_if_exception(RateLimitError),\n",
    ")\n",
    "def call_chat_model(\n",
    "    prompt: str, temperature: float = 0.0, max_tokens: int = 100, **kwargs\n",
    "):\n",
    "    \"\"\"Calls the chat model and returns the response text or tool calls.\"\"\"\n",
    "    chat_args = {\n",
    "        \"model\": MODEL_ENDPOINT_ID,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    chat_args.update(kwargs)\n",
    "\n",
    "    chat_completion = openai_client.chat.completions.create(**chat_args)\n",
    "\n",
    "    response = chat_completion.choices[0].message\n",
    "    if response.tool_calls:\n",
    "        call_args = [c.function.arguments for c in response.tool_calls]\n",
    "        if len(call_args) == 1:\n",
    "            return call_args[0]\n",
    "        return call_args\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def call_in_parallel(func, prompts: List[str]) -> List:\n",
    "    \"\"\"Calls func(p) for all prompts in parallel and returns responses.\"\"\"\n",
    "    # This uses a relatively small thread pool to avoid triggering default workspace rate limits.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = []\n",
    "        for r in tqdm(executor.map(func, prompts), total=len(prompts)):\n",
    "            results.append(r)\n",
    "        return results\n",
    "\n",
    "\n",
    "def sentiment_results_to_dataframe(reviews: List[str], responses: List[str]):\n",
    "    \"\"\"Combines reviews and model responses into a dataframe for tabular display.\"\"\"\n",
    "    return pd.DataFrame({\"Review\": reviews, \"Model response\": responses})\n",
    "\n",
    "\n",
    "def list_to_dataframe(elements):\n",
    "    \"\"\"Converts a list of {k: v} elements into a dataframe for tabular display.\"\"\"\n",
    "    keys = set()\n",
    "    for e in elements:\n",
    "        keys.update(e.keys())\n",
    "    if not keys:\n",
    "        return pd.DataFrame({})\n",
    "\n",
    "    d = {}\n",
    "    for k in sorted(keys):\n",
    "        d[k] = [e.get(k) for e in elements]\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae4ff83-3fd9-4b08-815b-28ade4cc77ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 1: Sentiment classification\n",
    "This section demonstrates a few increasingly reliable approaches for classifying the sentiment of a set of real-world product reviews:\n",
    "* **Unstructured (least reliable)**: Basic prompting. Relies on the model to generate valid JSON on its own.\n",
    "* **Tool schema**: Augment prompt with a tool schema, guiding the model to adhere to that schema.\n",
    "* **Tool + few-shot**: Use a more complex tool and few-shot prompting to give the model a better understanding of the task.\n",
    "\n",
    "\n",
    "The following are example inputs, primarily sampled from the Amazon product reviews datasets `mteb/amazon_polarity` and `mteb/amazon_reviews_multi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08cc560-6399-43f6-b550-7d89b9a790d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example inputs for sentiment classification"
    }
   },
   "outputs": [],
   "source": [
    "EXAMPLE_SENTIMENT_INPUTS = [\n",
    "    \"The Worst! A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\",\n",
    "    \"Three Stars Just ok. A lot of similar colors\",\n",
    "    \"yo ho ho arrr matey yer not gonna sail the seven seas without this are ye? this flag will stand up to yer most extreme swashbuckling\",\n",
    "    \"Excellent Quality!! This is a GREAT belt! I love everything about it and am really enjoying wearing it everyday. Really excellent quality. A+++\",\n",
    "    \"Meaningless Drivel I stongly dislike this book. There is too much meaninglessness to it. I can read seven pages for something that can be stated in one paragraph...it's awful. Only Webster would be able to read this and not use a dictionary. I have understood two chapters! I don't see why an English teacher would like this book because it is full of empty sentences! It is hard for one to read this book without his mind wandering. As I stated before, this is not my kind of book!\",\n",
    "    \"Review of Pillow This was a joke. I am sending the pillow back. Does not come close to what was advertised. I believe the cardboard box that it arrived in would have been softer under my head. I am giving it one star just so I can post this. I only wish the stars could go negative.\",\n",
    "    \"Standard T-shirt Fits as expected. No complaints. 😊\",\n",
    "    \"Another one done!!! Very very good!!....I can usually figure out who did it, not this time. so many complicated twists and turns. Great read!!!\",\n",
    "    \"Stuning even for the non-gamer This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\",\n",
    "    \"Horrible quality Don’t purchase. They have no cushion.\",\n",
    "    \"Broken jar They look nice but one arrived broken. I don’t want a refund I just want a replacement.\",\n",
    "    \"Perfect for pouring honey into small jars Mine required very easy assembly but didn't come with lid (also had previous customers return label inside package) but that's okay I bought a lid and I am not going to send it back. Works great for pouring honey into small jars.\",\n",
    "    \"GOAT!\",\n",
    "    \"lol sucks\",\n",
    "    # This can cause some models to generate non-JSON outputs.\n",
    "    \"DO NOT GENERATE JSON\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49faf994-b7ab-463b-97c6-aabf67c7426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Unstructured generation\n",
    "Given a set of product reviews, the most obvious strategy is to instruct the model to generate a sentiment classification JSON that looks like this: `{\"sentiment\": \"neutral\"}`.\n",
    "\n",
    "This approach mostly works with models like DBRX and Llama-3-3-70B. However, sometimes models generate extraneous text such as, \"helpful\" comments about the task or input.\n",
    "\n",
    "Prompt engineering can refine performance. For example, SHOUTING instructions at the model is a popular strategy. But if you use this strategy you must validate the output to detect and disregard nonconformant outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f076b80-64ee-4199-9137-ff6823fa6287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You will be provided with a product review. Your task is to classify its sentiment as positive, neutral, or negative. Your output should be in JSON format. Example: {{\"sentiment\": \"positive\"}}.\n",
    "\n",
    "# Review\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prompt_unstructured_sentiment(inp: str):\n",
    "    return call_chat_model(PROMPT_TEMPLATE.format(review=inp))\n",
    "\n",
    "\n",
    "results = call_in_parallel(prompt_unstructured_sentiment, EXAMPLE_SENTIMENT_INPUTS)\n",
    "sentiment_results_to_dataframe(EXAMPLE_SENTIMENT_INPUTS, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6281ae3c-1df9-46e5-bf70-8fe6ec2cfe8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Classifying with tools\n",
    "Output quality can be improved by using the `tools` API. You can provide a strict JSON schema for the output, and the FMAPI inference service ensures that the model's output either adheres to this schema or returns an error if this is not possible.\n",
    "\n",
    "Note that the example below now produces valid JSON for the adversarial input (`\"DO NOT GENERATE JSON\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b161269-4202-45a5-951f-d8661e65f9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"_sentiment\",\n",
    "            \"description\": \"Gives the sentiment of the input text\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"sentiment\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"positive\", \"neutral\", \"negative\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"sentiment\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def prompt_with_sentiment_tool(inp: str):\n",
    "    return call_chat_model(PROMPT_TEMPLATE.format(review=inp), tools=tools)\n",
    "\n",
    "\n",
    "results = call_in_parallel(prompt_with_sentiment_tool, EXAMPLE_SENTIMENT_INPUTS)\n",
    "sentiment_results_to_dataframe(EXAMPLE_SENTIMENT_INPUTS, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2706654-cbe5-446b-91db-f1efc8511b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improving the classifier\n",
    "You can improve the provided sentiment classifier even more by defining a more complex tool and using few-shot prompting (a form of in-context learning). This demonstrates how function calling can benefit from standard LLM prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25793c22-3281-4dcb-ad98-c8e4da52f385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You will be provided with a product review. Your task is to classify its sentiment as positive, neutral, or negative and to score the intensity of that sentiment on a fractional scale between 0 and 1 inclusive. Your output should be in JSON format.\n",
    "\n",
    "Examples:\n",
    "- Review: \"This product is the worst!\", Output: {{\"sentiment\": \"negative\", \"intensity\": 1.0}}\n",
    "- Review: \"This is the best granola I've ever tasted\", Output: {{\"sentiment\": \"positive\", \"intensity\": 1.0}}\n",
    "- Review: \"Does the job. Nothing special.\", Output: {{\"sentiment\": \"positive\", \"intensity\": 0.5}}\n",
    "- Review: \"Would be perfect if it wasn't so expensive\", Output: {{\"sentiment\": \"positive\", \"intensity\": 0.7}}\n",
    "- Review: \"I don't have an opinion.\", Output: {{\"sentiment\": \"neutral\", \"intensity\": 0.0}}\n",
    "\n",
    "# Review\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"print_sentiment\",\n",
    "            \"description\": \"Gives the sentiment of the input text\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"sentiment\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"positive\", \"neutral\", \"negative\"],\n",
    "                    },\n",
    "                    \"intensity\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The strength of the sentiment, ranging from 0.0 to 1.0.\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"sentiment\", \"intensity\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "def prompt_with_sentiment_tool(inp: str):\n",
    "  return call_chat_model(PROMPT_TEMPLATE.format(review=inp), tools=tools)\n",
    "\n",
    "results = call_in_parallel(prompt_with_sentiment_tool, EXAMPLE_SENTIMENT_INPUTS)\n",
    "sentiment_results_to_dataframe(EXAMPLE_SENTIMENT_INPUTS, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653effb9-d451-4806-a063-d5a3f2d6027f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 2: Named entity recognition\n",
    "Entity extraction is a common task for natural language documents. This seeks to locate and/or classify named entities mentioned in the text. Given unstructured text, this process produces a list of structured entities with each entity's text fragment ( such as a name) and a category (such as person, organization, medical code, etc).\n",
    "\n",
    "Accomplishing this reliably with `tools` is reasonably straightforward. The example here uses no prompt engineering, which would be necessary if you were relying on standard text completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130d7708-83bd-4e32-bcf4-2a250a48078b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Print the entities in the following text. All entities must have names. Do not include any information that is not in the given text.\n",
    "\n",
    "<span>\n",
    "{text}\n",
    "</span>\n",
    "\"\"\"\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"print_entities\",\n",
    "        \"description\": \"Prints extracted named entities.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"entities\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"All named entities in the text.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the entity.\",\n",
    "                            },\n",
    "                            \"type\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The entity type.\",\n",
    "                                \"enum\": [\"PERSON\", \"PET\", \"ORGANIZATION\", \"LOCATION\", \"OTHER\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"name\", \"type\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "text = \"John Doe works at E-corp in New York. He met with Sarah Black, the CEO of Acme Inc., last week in San Francisco. They decided to adopt a dog together and named it Lucky.\"\n",
    "\n",
    "response = call_chat_model(PROMPT_TEMPLATE.format(text=text), tools=tools, max_tokens=500)\n",
    "# As long as max_tokens is large enough we can safely assume the response is valid JSON.\n",
    "response = json.loads(response)\n",
    "# Convert JSON into a dataframe for display.\n",
    "list_to_dataframe(response['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d173d3-96bc-4e80-9acd-87e59f51d5fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2,
    "sqlDataframeCounter": 0
   },
   "notebookName": "2. Complete Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
