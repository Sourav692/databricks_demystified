{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17ab104-8571-4e0e-8d18-842d26004608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1431c125-aa8e-49be-942a-aa3008748e76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Demo Tables for the Demo"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ===================================================================\n",
    "# # CREATE DEMO SOURCE TABLES FOR SCD OPERATIONS\n",
    "# # Run this in a standard notebook before creating the DLT pipeline\n",
    "# # ===================================================================\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import *\n",
    "# from datetime import datetime\n",
    "\n",
    "# print(\"Creating demo source tables for SCD operations...\")\n",
    "\n",
    "# # ===================================================================\n",
    "# # Customer Master Data (Main Source)\n",
    "# # ===================================================================\n",
    "\n",
    "# initial_customers = [\n",
    "#     Row(customer_id=1, name=\"Alice Johnson\", email=\"alice@email.com\", city=\"New York\", \n",
    "#         tier=\"Gold\", status=\"Active\", last_updated=\"2024-01-01 10:00:00\"),\n",
    "#     Row(customer_id=2, name=\"Bob Smith\", email=\"bob@email.com\", city=\"Chicago\", \n",
    "#         tier=\"Silver\", status=\"Active\", last_updated=\"2024-01-01 10:00:00\"),\n",
    "#     Row(customer_id=3, name=\"Carol Davis\", email=\"carol@email.com\", city=\"Miami\", \n",
    "#         tier=\"Bronze\", status=\"Active\", last_updated=\"2024-01-01 10:00:00\"),\n",
    "#     Row(customer_id=4, name=\"David Wilson\", email=\"david@email.com\", city=\"Seattle\", \n",
    "#         tier=\"Gold\", status=\"Active\", last_updated=\"2024-01-01 10:00:00\"),\n",
    "# ]\n",
    "\n",
    "# customers_df = spark.createDataFrame(initial_customers)\n",
    "# customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_university.dlt.scd_customers_source\")\n",
    "\n",
    "# # ===================================================================\n",
    "# # Change Data Capture (CDC) Events Table\n",
    "# # ===================================================================\n",
    "\n",
    "# # This table simulates incoming change events for realistic SCD processing\n",
    "# initial_cdc_events = [\n",
    "#     Row(customer_id=1, name=\"Alice Johnson\", email=\"alice@email.com\", city=\"New York\", \n",
    "#         tier=\"Gold\", operation=\"INSERT\", event_timestamp=\"2024-01-01 10:00:00\", change_sequence=1),\n",
    "#     Row(customer_id=2, name=\"Bob Smith\", email=\"bob@email.com\", city=\"Chicago\", \n",
    "#         tier=\"Silver\", operation=\"INSERT\", event_timestamp=\"2024-01-01 10:00:00\", change_sequence=2),\n",
    "#     Row(customer_id=3, name=\"Carol Davis\", email=\"carol@email.com\", city=\"Miami\", \n",
    "#         tier=\"Bronze\", operation=\"INSERT\", event_timestamp=\"2024-01-01 10:00:00\", change_sequence=3),\n",
    "#     Row(customer_id=4, name=\"David Wilson\", email=\"david@email.com\", city=\"Seattle\", \n",
    "#         tier=\"Gold\", operation=\"INSERT\", event_timestamp=\"2024-01-01 10:00:00\", change_sequence=4),\n",
    "# ]\n",
    "\n",
    "# cdc_df = spark.createDataFrame(initial_cdc_events)\n",
    "# cdc_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n",
    "\n",
    "# print(\"Demo source tables created successfully!\")\n",
    "# print(\"\\nCustomers Source:\")\n",
    "# spark.sql(\"SELECT * FROM data_university.dlt.scd_customers_source\").show()\n",
    "# print(\"\\nCDC Events:\")\n",
    "# spark.sql(\"SELECT * FROM data_university.dlt.scd_cdc_events\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b6c6f9-d712-4a49-a3ac-f27ad1e14e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DLT PIPELINE: COMPREHENSIVE SCD OPERATIONS DEMO\n",
    "# Use this notebook as source code for your DLT pipeline\n",
    "# ===================================================================\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Pipeline parameters\n",
    "catalog = \"data_university\"\n",
    "schema = \"dlt\"\n",
    "customers_source = f\"{catalog}.{schema}.scd_customers_source\"\n",
    "cdc_source = f\"{catalog}.{schema}.scd_cdc_events\"\n",
    "\n",
    "print(f\"Reading from customers source: {customers_source}\")\n",
    "print(f\"Reading from CDC source: {cdc_source}\")\n",
    "\n",
    "# ===================================================================\n",
    "# BRONZE LAYER: Raw Data Ingestion\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers_raw\",\n",
    "    comment=\"Raw customer data for SCD processing\"\n",
    ")\n",
    "def bronze_customers_raw():\n",
    "    \"\"\"Streaming table for raw customer data ingestion\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(customers_source)\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_cdc_events\",\n",
    "    comment=\"Raw CDC events for change data capture\"\n",
    ")\n",
    "def bronze_cdc_events():\n",
    "    \"\"\"Streaming table for CDC events\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(cdc_source)\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# SCD TYPE 1: Overwrite Changes (No History Preservation)\n",
    "# ===================================================================\n",
    "\n",
    "# Create target table for SCD1\n",
    "dlt.create_streaming_table(\n",
    "    name=\"silver_customers_scd1\",\n",
    "    comment=\"SCD Type 1 - Overwrites changes, no history tracking\"\n",
    ")\n",
    "\n",
    "# Apply SCD1 logic using DLT's apply_changes\n",
    "dlt.apply_changes(\n",
    "    target=\"silver_customers_scd1\",\n",
    "    source=\"bronze_cdc_events\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=col(\"change_sequence\"),\n",
    "    apply_as_deletes = expr(\"operation = 'DELETE'\"),\n",
    "    apply_as_truncates = expr(\"operation = 'TRUNCATE'\"),\n",
    "    except_column_list=[\"operation\", \"event_timestamp\", \"change_sequence\"],\n",
    "    stored_as_scd_type=\"1\"\n",
    ")\n",
    "\n",
    "  \n",
    "\n",
    "# ===================================================================\n",
    "# SCD TYPE 2: Preserve Complete History\n",
    "# ===================================================================\n",
    "\n",
    "# Create target table for SCD2\n",
    "dlt.create_streaming_table(\n",
    "    name=\"silver_customers_scd2\",\n",
    "    comment=\"SCD Type 2 - Preserves complete history with effective dates\"\n",
    ")\n",
    "\n",
    "# Apply SCD2 logic using DLT's apply_changes\n",
    "dlt.apply_changes(\n",
    "    target=\"silver_customers_scd2\",\n",
    "    source=\"bronze_cdc_events\",\n",
    "    keys=[\"customer_id\"],\n",
    "    sequence_by=col(\"change_sequence\"),\n",
    "    apply_as_deletes=expr(\"operation = 'DELETE'\"),\n",
    "    except_column_list=[\"operation\", \"event_timestamp\", \"change_sequence\"],\n",
    "    stored_as_scd_type=\"2\"\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# GOLD LAYER: Business Analytics on SCD Data\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_customer_current_state\",\n",
    "    comment=\"Current active customer snapshot from SCD2\"\n",
    ")\n",
    "def gold_customer_current_state():\n",
    "    \"\"\"Current snapshot of customers from SCD Type 2 table\"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.silver_customers_scd2\")\n",
    "        .filter(col(\"__END_AT\").isNull())  # Current records have null end date\n",
    "        .select(\"customer_id\", \"name\", \"email\", \"city\", \"tier\", \"__START_AT\")\n",
    "        .orderBy(\"customer_id\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"gold_customer_history_analysis\",\n",
    "    comment=\"Analysis of customer changes over time\"\n",
    ")\n",
    "def gold_customer_history_analysis():\n",
    "    \"\"\"Analytics showing customer change patterns\"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.silver_customers_scd2\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            first(\"name\").alias(\"customer_name\"),\n",
    "            count(\"*\").alias(\"total_versions\"),\n",
    "            min(\"__START_AT\").alias(\"first_seen\"),\n",
    "            max(\"__START_AT\").alias(\"last_changed\"),\n",
    "            sum(when(col(\"__END_AT\").isNull(), 1).otherwise(0)).alias(\"active_versions\")\n",
    "        )\n",
    "        .orderBy(\"customer_id\")\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# DEMONSTRATION TABLES: Delete and Truncate Operations\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"demo_scd1_with_deletes\",\n",
    "    comment=\"SCD1 table demonstrating delete operations\"\n",
    ")\n",
    "def demo_scd1_with_deletes():\n",
    "    \"\"\"Demonstrates logical delete in SCD1 by filtering out deleted records\"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.silver_customers_scd1\")\n",
    "        .filter(col(\"customer_id\") != 999)  # Simulate delete of customer 999\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"demo_scd2_soft_deletes\",\n",
    "    comment=\"SCD2 table showing soft delete behavior\"\n",
    ")\n",
    "def demo_scd2_soft_deletes():\n",
    "    \"\"\"Shows how SCD2 handles soft deletes with end dates\"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.silver_customers_scd2\")\n",
    "        .select(\n",
    "            \"customer_id\", \"name\", \"city\", \"tier\",\n",
    "            \"__START_AT\", \"__END_AT\", \n",
    "            when(col(\"__END_AT\").isNull(), \"Active\").otherwise(\"Deleted\").alias(\"record_status\")\n",
    "        )\n",
    "        .orderBy(\"customer_id\", \"__START_AT\")\n",
    "    )\n",
    "\n",
    "# ===================================================================\n",
    "# UTILITY VIEWS\n",
    "# ===================================================================\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"view_change_summary\",\n",
    "    comment=\"Summary of all change events processed\"\n",
    ")\n",
    "def view_change_summary():\n",
    "    \"\"\"Non-materialized view showing change event summary\"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.bronze_cdc_events\")\n",
    "        .groupBy(\"operation\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"event_count\"),\n",
    "            min(\"event_timestamp\").alias(\"first_event\"),\n",
    "            max(\"event_timestamp\").alias(\"last_event\")\n",
    "        )\n",
    "        .orderBy(\"operation\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906cbba6-4f87-4312-9e63-60fc2b94f530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SCD1 Changes - Updates and New Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0520743c-4382-4f01-bb08-49e0fa1644fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # DEMONSTRATE SCD1 BEHAVIOR\n",
    "# # Run this after initial pipeline execution\n",
    "# # ===================================================================\n",
    "\n",
    "# print(\"Adding SCD1 changes - updates and new records...\")\n",
    "\n",
    "# # Add change events for SCD1 demonstration\n",
    "# scd1_changes = [\n",
    "#     # Update Alice's tier (SCD1 will overwrite)\n",
    "#     Row(customer_id=1, name=\"Alice Johnson\", email=\"alice.johnson@newemail.com\", city=\"Boston\", \n",
    "#         tier=\"Platinum\", operation=\"UPDATE\", event_timestamp=\"2024-01-15 09:00:00\", change_sequence=5),\n",
    "    \n",
    "#     # Update Bob's city (SCD1 will overwrite)\n",
    "#     Row(customer_id=2, name=\"Bob Smith\", email=\"bob@email.com\", city=\"Denver\", \n",
    "#         tier=\"Gold\", operation=\"UPDATE\", event_timestamp=\"2024-01-15 10:00:00\", change_sequence=6),\n",
    "    \n",
    "#     # New customer\n",
    "#     Row(customer_id=5, name=\"Eva Brown\", email=\"eva@email.com\", city=\"Portland\", \n",
    "#         tier=\"Silver\", operation=\"INSERT\", event_timestamp=\"2024-01-15 11:00:00\", change_sequence=7),\n",
    "# ]\n",
    "\n",
    "# scd1_df = spark.createDataFrame(scd1_changes)\n",
    "# scd1_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n",
    "\n",
    "# print(\"SCD1 changes added. Re-run DLT pipeline to see overwrite behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a855b20-179a-48bb-a4e0-f8b9bf845f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD2 Changes - Historical Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24967013-1c32-4d2e-b2c8-b1cd5b51fb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # DEMONSTRATE SCD2 BEHAVIOR\n",
    "# # Run this after SCD1 changes and pipeline update\n",
    "# # ===================================================================\n",
    "\n",
    "# print(\"Adding SCD2 changes - historical tracking...\")\n",
    "\n",
    "# # Add more change events to demonstrate SCD2 history preservation\n",
    "# scd2_changes = [\n",
    "#     # Multiple changes for Carol to show history\n",
    "#     Row(customer_id=3, name=\"Carol Davis\", email=\"carol@email.com\", city=\"Orlando\", \n",
    "#         tier=\"Silver\", operation=\"UPDATE\", event_timestamp=\"2024-01-20 09:00:00\", change_sequence=8),\n",
    "    \n",
    "#     Row(customer_id=3, name=\"Carol Davis-Smith\", email=\"carol.smith@email.com\", city=\"Orlando\", \n",
    "#         tier=\"Gold\", operation=\"UPDATE\", event_timestamp=\"2024-01-25 10:00:00\", change_sequence=9),\n",
    "    \n",
    "#     # Change for David\n",
    "#     Row(customer_id=4, name=\"David Wilson\", email=\"david.wilson@email.com\", city=\"Portland\", \n",
    "#         tier=\"Platinum\", operation=\"UPDATE\", event_timestamp=\"2024-01-22 14:00:00\", change_sequence=10),\n",
    "# ]\n",
    "\n",
    "# scd2_df = spark.createDataFrame(scd2_changes)\n",
    "# scd2_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n",
    "\n",
    "# print(\"SCD2 changes added. Re-run DLT pipeline to see history preservation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd8c1d9-8503-45fa-9bb0-6a204a090981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "D. Delete Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11753b6-6811-4918-8f88-527957ca9bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # DEMONSTRATE DELETE OPERATIONS\n",
    "# # ===================================================================\n",
    "\n",
    "# print(\"Adding delete operations...\")\n",
    "\n",
    "# # Add delete events\n",
    "# delete_events = [\n",
    "#     # Soft delete for customer 4\n",
    "#     Row(customer_id=4, name=\"David Wilson\", email=\"david.wilson@email.com\", city=\"Portland\", \n",
    "#         tier=\"Platinum\", operation=\"DELETE\", event_timestamp=\"2024-01-30 15:00:00\", change_sequence=11),\n",
    "# ]\n",
    "\n",
    "# delete_df = spark.createDataFrame(delete_events)\n",
    "# delete_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n",
    "\n",
    "# print(\"Delete events added. Re-run DLT pipeline to see delete handling.\")\n",
    "# print(\"- SCD1: Record will be removed\")\n",
    "# print(\"- SCD2: Record will have __END_AT populated (soft delete)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90076957-c517-44a3-9fc3-4dd337fd0255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Backfilling Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf45b37-9e85-4c15-9f81-7a1cb182b4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # DEMONSTRATE BACKFILLING FOR SCD2\n",
    "# # ===================================================================\n",
    "\n",
    "# print(\"Adding historical data for backfilling...\")\n",
    "\n",
    "# # Add historical events with earlier timestamps\n",
    "# backfill_events = [\n",
    "#     # Historical data for Alice before her first recorded change\n",
    "#     Row(customer_id=1, name=\"Alice Smith\", email=\"alice.smith@oldmail.com\", city=\"Philadelphia\", \n",
    "#         tier=\"Silver\", operation=\"INSERT\", event_timestamp=\"2023-12-01 08:00:00\", change_sequence=0),\n",
    "    \n",
    "#     # Historical change for Bob\n",
    "#     Row(customer_id=2, name=\"Robert Smith\", email=\"robert@email.com\", city=\"Milwaukee\", \n",
    "#         tier=\"Bronze\", operation=\"INSERT\", event_timestamp=\"2023-12-15 09:00:00\", change_sequence=1),\n",
    "# ]\n",
    "\n",
    "# backfill_df = spark.createDataFrame(backfill_events)\n",
    "# backfill_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n",
    "\n",
    "# print(\"Historical events added for backfilling!\")\n",
    "# print(\"Re-run DLT pipeline to see how SCD2 handles out-of-order data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0fed03-4c75-41a8-b09a-40ef0464db99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add a TRUNCATE Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78aebc5-4e53-422a-b7be-3485bca32486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "\n",
    "# truncate_event = [\n",
    "#     Row(customer_id=1, name=\"Sourav\", email=\"sourav@oldmail.com\", city=\"Bangalore\", tier=\"Silver\",\n",
    "#         operation=\"TRUNCATE\", event_timestamp=\"2024-02-02 09:00:00\", change_sequence=4)\n",
    "# ]\n",
    "# spark.createDataFrame(truncate_event).write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c9ef07-cf58-4046-b74f-faf4cb48a987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new_data = [\n",
    "#     Row(customer_id=4, name=\"David Wilson\", email=\"david@email.com\", city=\"Seattle\", tier=\"Gold\",\n",
    "#         operation=\"INSERT\", event_timestamp=\"2024-02-01 09:01:00\", change_sequence=5),\n",
    "#     Row(customer_id=5, name=\"Eva Brown\", email=\"eva@email.com\", city=\"Portland\", tier=\"Silver\",\n",
    "#         operation=\"INSERT\", event_timestamp=\"2024-02-01 09:02:00\", change_sequence=6)\n",
    "# ]\n",
    "# spark.createDataFrame(new_data).write.format(\"delta\").mode(\"append\").saveAsTable(\"data_university.dlt.scd_cdc_events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c94a1511-a185-48b8-8fe0-de582f398c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. DLT SCD Operation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
