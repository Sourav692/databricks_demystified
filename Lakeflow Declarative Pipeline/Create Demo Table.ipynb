{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42cff86-7a06-481c-a42c-3a64db51ce97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS data_university.dlt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98c6363-2474-4693-88aa-628cce22db8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo Table for First DLT Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# SIMPLE DEMO DELTA TABLE CREATION\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# -- Sales Table (Very Simple Structure)\n",
    "sales_data = [\n",
    "    Row(order_id=1, customer_id=1, amount=100.0, order_date=\"2024-01-01\"),\n",
    "    Row(order_id=2, customer_id=2, amount=200.0, order_date=\"2024-01-02\"),\n",
    "    Row(order_id=3, customer_id=1, amount=150.0, order_date=\"2024-01-03\"),\n",
    "    Row(order_id=4, customer_id=3, amount=120.0, order_date=\"2024-01-03\"),\n",
    "    Row(order_id=5, customer_id=2, amount=90.0,  order_date=\"2024-01-04\"),\n",
    "]\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_university.dlt.demo_sales_source\")\n",
    "\n",
    "# -- Customers Table (Very Simple Structure)\n",
    "customers_data = [\n",
    "    Row(customer_id=1, customer_name=\"Alice\"),\n",
    "    Row(customer_id=2, customer_name=\"Bob\"),\n",
    "    Row(customer_id=3, customer_name=\"Carol\"),\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_university.dlt.demo_customers_source\")\n",
    "\n",
    "print(\"Demo tables created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d1348f-beec-40b3-a72c-e150fefc7c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DROP SCHEMA IF EXISTS data_university.dl1 CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91238bc-b6d1-4618-ad33-fcf658a846f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo Table for First DLT Pipeline - 2nd Set"
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DEMO TABLE CREATION SCRIPT\n",
    "# Run this in a separate notebook before creating the DLT pipeline\n",
    "# ===================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col  # Explicit import for col function\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import builtins  # To access Python's built-in round function\n",
    "\n",
    "# Initialize Spark session (already available in Databricks as 'spark')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define your target catalog and schema (update these values)\n",
    "catalog_name = \"data_university\"  # Replace with your catalog\n",
    "schema_name = \"dlt01\"  # Replace with your schema\n",
    "\n",
    "# Create schema if it doesn't exist (for Unity Catalog)\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "# ===================================================================\n",
    "# Create Sales Demo Data\n",
    "# ===================================================================\n",
    "\n",
    "# Define schema for sales data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"order_date\", TimestampType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Generate sample sales data\n",
    "sales_data = []\n",
    "base_date = datetime(2024, 1, 1)\n",
    "products = [\n",
    "    (\"PROD_001\", \"Laptop\", \"Electronics\"),\n",
    "    (\"PROD_002\", \"Mouse\", \"Accessories\"),\n",
    "    (\"PROD_003\", \"Keyboard\", \"Accessories\"),\n",
    "    (\"PROD_004\", \"Monitor\", \"Electronics\"),\n",
    "    (\"PROD_005\", \"Headphones\", \"Electronics\"),\n",
    "    (\"PROD_006\", \"Tablet\", \"Electronics\"),\n",
    "    (\"PROD_007\", \"Phone\", \"Electronics\"),\n",
    "    (\"PROD_008\", \"Charger\", \"Accessories\")\n",
    "]\n",
    "\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "\n",
    "# Generate 200 sample sales records\n",
    "for i in range(200):\n",
    "    product = random.choice(products)\n",
    "    order_date = base_date + timedelta(days=random.randint(0, 180))\n",
    "    \n",
    "    sales_data.append((\n",
    "        f\"ORD_{i+1:05d}\",  # order_id\n",
    "        f\"CUST_{random.randint(1, 50):03d}\",  # customer_id\n",
    "        product[0],  # product_id\n",
    "        product[1],  # product_name\n",
    "        product[2],  # category\n",
    "        random.randint(1, 5),  # quantity\n",
    "        builtins.round(random.uniform(10, 500), 2),  # unit_price\n",
    "        order_date,  # order_date\n",
    "        random.choice(regions)  # region\n",
    "    ))\n",
    "\n",
    "# Create sales DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Add calculated column for total amount\n",
    "sales_df_enhanced = sales_df.withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\"))\n",
    "\n",
    "# Write to Delta table\n",
    "sales_table_name = f\"{catalog_name}.{schema_name}.demo_sales_source\"\n",
    "sales_df_enhanced.write.format(\"delta\").mode(\"overwrite\").saveAsTable(sales_table_name)\n",
    "\n",
    "# ===================================================================\n",
    "# Create Customer Demo Data\n",
    "# ===================================================================\n",
    "\n",
    "customer_data = []\n",
    "for i in range(50):\n",
    "    customer_data.append((\n",
    "        f\"CUST_{i+1:03d}\",  # customer_id\n",
    "        f\"Customer_{i+1}\",  # customer_name\n",
    "        random.choice(regions),  # region\n",
    "        random.choice([\"Premium\", \"Standard\", \"Basic\"]),  # tier\n",
    "        random.randint(25, 65)  # age\n",
    "    ))\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"tier\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "\n",
    "# Write customer table\n",
    "customer_table_name = f\"{catalog_name}.{schema_name}.demo_customers_source\"\n",
    "customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(customer_table_name)\n",
    "\n",
    "print(\"Demo tables created successfully!\")\n",
    "print(f\"Sales table: {sales_table_name} - {sales_df_enhanced.count()} records\")\n",
    "print(f\"Customer table: {customer_table_name} - {customer_df.count()} records\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n--- Sample Sales Data ---\")\n",
    "sales_df_enhanced.show(5)\n",
    "\n",
    "print(\"\\n--- Sample Customer Data ---\")\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f6a07f-8c1e-4c27-a5d9-1741d2b4cdb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7788710225205955,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Create Demo Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
