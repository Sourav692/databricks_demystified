{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a07d6e0-545d-4b21-8256-a8e6d5a5bb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9074c96c-c5e6-41a3-8059-4a666c344ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfb94ec-94b9-4d6a-b676-b62b0cdd27b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, sum as pysum, desc\n",
    "\n",
    "# --- Pipeline parameters for catalog/schema (default to 'main' and 'default') ---\n",
    "# catalog = spark.conf.get(\"source_catalog\", \"main\")\n",
    "# schema = spark.conf.get(\"source_schema\", \"default\")\n",
    "sales_table = \"data_university.dlt.demo_sales_source\"\n",
    "customers_table = \"data_university.dlt.demo_customers_source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de5e7c7-fd1d-4daa-8ae5-0ca3e77b6d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------- BRONZE: Raw Sales Data (streaming table, but you can use batch too) ---------\n",
    "@dlt.table(\n",
    "    name=\"bronze_sales\",\n",
    "    comment=f\"Raw sales data from {sales_table}\"\n",
    ")\n",
    "def bronze_sales():\n",
    "    # For demo datasets, use batch read for simplicity; use readStream for real streaming sources\n",
    "    return spark.read.format(\"delta\").table(sales_table)\n",
    "\n",
    "# --------- BRONZE: Raw Customers Data (batch read as materialized view) ---------\n",
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=f\"Raw customers data from {customers_table}\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return spark.read.format(\"delta\").table(customers_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4159be6-7928-40f3-ae44-ec629a613cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------- SILVER: Join Sales with Customer Names ---------\n",
    "@dlt.table(\n",
    "    name=\"silver_sales_with_customer\",\n",
    "    comment=\"Sales data with customer names\"\n",
    ")\n",
    "def silver_sales_with_customer():\n",
    "    sales = spark.read.table(\"LIVE.bronze_sales\")\n",
    "    customers = spark.read.table(\"LIVE.bronze_customers\")\n",
    "    return sales.join(customers, \"customer_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cdabb3-8936-46b1-9641-573619718d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------- GOLD: Aggregate Sales by Customer ---------\n",
    "@dlt.table(\n",
    "    name=\"gold_sales_by_customer\",\n",
    "    comment=\"Total sales amount by customer\"\n",
    ")\n",
    "def gold_sales_by_customer():\n",
    "    df = spark.read.table(\"LIVE.silver_sales_with_customer\")\n",
    "    return (df.groupBy(\"customer_id\", \"customer_name\")\n",
    "              .agg(pysum(\"amount\").alias(\"total_amount\"))\n",
    "              .orderBy(desc(\"total_amount\")))\n",
    "\n",
    "# --------- GOLD: Daily Sales Summary ---------\n",
    "@dlt.table(\n",
    "    name=\"gold_daily_sales\",\n",
    "    comment=\"Total sales per day\"\n",
    ")\n",
    "def gold_daily_sales():\n",
    "    df = spark.read.table(\"LIVE.bronze_sales\")\n",
    "    return (df.groupBy(\"order_date\")\n",
    "              .agg(pysum(\"amount\").alias(\"total_amount\"))\n",
    "              .orderBy(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd2ab21-0d6c-467b-a2c5-4b23b7c7a1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. First DLT Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
