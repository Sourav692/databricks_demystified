{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4171dc6-90a4-49f7-8add-632313b80363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Setting Up the Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13627fb0-3459-4a23-b8f3-f0e9e73ecc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Setup script for generating sample CDC data\n",
    "# catalog = \"data_university\"\n",
    "# schema = dbName = db = \"lakeflow\"\n",
    "# volume_name = \"raw_data\"\n",
    "\n",
    "# # Create catalog, schema, and volume\n",
    "# spark.sql(f'CREATE CATALOG IF NOT EXISTS `{catalog}`')\n",
    "# spark.sql(f'USE CATALOG `{catalog}`')\n",
    "# spark.sql(f'CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`')\n",
    "# spark.sql(f'USE SCHEMA `{schema}`')\n",
    "# spark.sql(f'CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume_name}`')\n",
    "\n",
    "# volume_folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "# # Generate sample customer data with CDC operations\n",
    "# from pyspark.sql import functions as F\n",
    "# from faker import Faker\n",
    "# from collections import OrderedDict\n",
    "# import uuid\n",
    "# import random\n",
    "\n",
    "# fake = Faker()\n",
    "# fake_firstname = F.udf(fake.first_name)\n",
    "# fake_lastname = F.udf(fake.last_name)\n",
    "# fake_email = F.udf(fake.ascii_company_email)\n",
    "# fake_date = F.udf(lambda: fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "# fake_address = F.udf(fake.address)\n",
    "\n",
    "# # Define CDC operations with realistic distribution\n",
    "# operations = OrderedDict([(\"APPEND\", 0.5), (\"DELETE\", 0.1), (\"UPDATE\", 0.3), (None, 0.01)])\n",
    "# fake_operation = F.udf(lambda: fake.random_elements(elements=operations, length=1)[0])\n",
    "# fake_id = F.udf(lambda: str(uuid.uuid4()) if random.uniform(0, 1) < 0.98 else None)\n",
    "\n",
    "# # Generate sample dataset\n",
    "# df = spark.range(0, 100000).repartition(100)\n",
    "# df = df.withColumn(\"id\", fake_id())\n",
    "# df = df.withColumn(\"firstname\", fake_firstname())\n",
    "# df = df.withColumn(\"lastname\", fake_lastname())\n",
    "# df = df.withColumn(\"email\", fake_email())\n",
    "# df = df.withColumn(\"address\", fake_address())\n",
    "# df = df.withColumn(\"operation\", fake_operation())\n",
    "# df_customers = df.withColumn(\"operation_date\", fake_date())\n",
    "\n",
    "# # Save the sample data\n",
    "# df_customers.repartition(100).write.format(\"json\").mode(\"overwrite\").save(volume_folder + \"/customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "469132ea-7d8e-4d4d-8d17-16f018badea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Bronze Layer - Raw Data Ingestion with Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0034c362-9735-477b-b4d6-03852d36481f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dlt import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create the target bronze table\n",
    "dlt.create_streaming_table(\n",
    "    \"customers_cdc_bronze\",\n",
    "    comment=\"New customer data incrementally ingested from cloud object storage landing zone\"\n",
    ")\n",
    "\n",
    "# Create an Append Flow to ingest the raw data into the bronze table\n",
    "@append_flow(\n",
    "    target=\"customers_cdc_bronze\",\n",
    "    name=\"customers_bronze_ingest_flow\"\n",
    ")\n",
    "def customers_bronze_ingest_flow():\n",
    "    return (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .load(\"/Volumes/data_university/lakeflow/raw_data/customers/\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd7e4e3-58ec-4059-a918-4b3a4636d567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Silver Layer - Data Quality and Cleansing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a352725a-5c17-4189-ad71-daca566c0a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming table with data quality expectations\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customers_cdc_clean\",\n",
    "    expect_all_or_drop={\n",
    "        \"no_rescued_data\": \"_rescued_data IS NULL\",\n",
    "        \"valid_id\": \"id IS NOT NULL\", \n",
    "        \"valid_operation\": \"operation IN ('APPEND', 'DELETE', 'UPDATE')\"\n",
    "    }\n",
    ")\n",
    "\n",
    "@append_flow(\n",
    "    target=\"customers_cdc_clean\",\n",
    "    name=\"customers_cdc_clean_flow\"\n",
    ")\n",
    "def customers_cdc_clean_flow():\n",
    "    return (\n",
    "        dlt.read_stream(\"customers_cdc_bronze\")\n",
    "            .select(\"address\", \"email\", \"id\", \"firstname\", \"lastname\", \n",
    "                   \"operation\", \"operation_date\", \"_rescued_data\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1796a24f-6984-4ee3-816b-6f7cd8af9888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Materialized Customer Table with AUTO CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de6b24d4-87c2-4aa4-bea3-8bd864cc2eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the target customer table\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customers\", \n",
    "    comment=\"Clean, materialized customers\"\n",
    ")\n",
    "\n",
    "# Create AUTO CDC flow to process changes\n",
    "dlt.create_auto_cdc_flow(\n",
    "    target=\"customers\",                    # The customer table being materialized\n",
    "    source=\"customers_cdc_clean\",          # The incoming CDC stream\n",
    "    keys=[\"id\"],                          # Primary key for matching rows\n",
    "    sequence_by=col(\"operation_date\"),     # Order operations by timestamp\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=expr(\"operation = 'DELETE'\"),  # Handle DELETE operations\n",
    "    except_column_list=[\"operation\", \"operation_date\", \"_rescued_data\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68b11c88-1f55-4851-873b-a7100b397bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Slowly Changing Dimension Type 2 (SCD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804cc149-e20e-4980-9cfd-fd3b77117025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the SCD2 history table\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customers_history\", \n",
    "    comment=\"Slowly Changing Dimension Type 2 for customers\"\n",
    ")\n",
    "\n",
    "# Create AUTO CDC flow with SCD2 enabled\n",
    "dlt.create_auto_cdc_flow(\n",
    "    target=\"customers_history\",\n",
    "    source=\"customers_cdc_clean\",\n",
    "    keys=[\"id\"],\n",
    "    sequence_by=col(\"operation_date\"),\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=expr(\"operation = 'DELETE'\"),\n",
    "    except_column_list=[\"operation\", \"operation_date\", \"_rescued_data\"],\n",
    "    stored_as_scd_type=\"2\"  # Enable SCD2 to store individual updates\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24783d23-0b26-4be8-8007-908edb117c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Gold Layer - Business Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d6bfc1e-881f-4858-8682-ae50ca68d958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"customers_history_agg\",\n",
    "    comment=\"Aggregated customer history for analytics\"\n",
    ")\n",
    "def customers_history_agg():\n",
    "    return (\n",
    "        dlt.read(\"customers_history\")\n",
    "            .groupBy(\"id\")\n",
    "            .agg(\n",
    "                count(\"address\").alias(\"address_count\"),\n",
    "                count(\"email\").alias(\"email_count\"), \n",
    "                count(\"firstname\").alias(\"firstname_count\"),\n",
    "                count(\"lastname\").alias(\"lastname_count\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8ec1e2-97e1-4579-b5ec-af6dd888ceeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT count(*) FROM data_university.lakeflow.customers_cdc_clean where operation = 'DELETE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddfc0ade-32a8-4702-aa8e-7c803586cecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT count(*) FROM data_university.lakeflow.customers_cdc_clean where operation NOT LIKE 'DELETE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3635daaa-dec1-4610-b1d8-caeafee01e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2038282820252929,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Build an ETL pipeline using change data capture with Lakeflow Declarative Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
