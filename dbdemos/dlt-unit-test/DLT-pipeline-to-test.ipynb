{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801bc985-dcf9-4287-8474-b849e77ab83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta live table - Unit testing\n",
    "\n",
    "## Why testing?\n",
    "\n",
    "Deploying tests on your DLT pipelines will guarantee that your ingestion is always stable and future proof.\n",
    "\n",
    "The tests can be deployed as part of traditional CI/CD pipeline and can be run before a new version deployment, ensuring that a new version won't introduce a regression.\n",
    "\n",
    "This is critical in the Lakehouse ecosystem, as the data we produce will then leveraged downstream:\n",
    "\n",
    "* By Data Analyst for reporting/BI\n",
    "* By Data Scientists to build ML model for downstream applications\n",
    "\n",
    "## Unit testing strategy with DLT\n",
    "\n",
    "Delta Live Table logic can be unit tested leveraging Expectation.\n",
    "\n",
    "At a high level, the DLT pipelines can be constructed as following:\n",
    "\n",
    "* The ingestion step (first step of the pipeline on the left) is written in a separate notebook. This correspond to the left **green** (prod) and **blue** (test) input sources.\n",
    "   * The Production pipeline is defined with the PROD ingestion notebook:[./ingestion_profile/DLT-ingest_prod]($./ingestion_profile/DLT-ingest_prod) and connects to the live datasource (ex: kafka server, staging blob storage)\n",
    "   * The Test pipeline (only used to run the unit test) is defined with the TEST ingestion notebook: [./ingestion_profile/DLT-ingest_test]($./ingestion_profile/DLT-ingest_test) and can consume from local files used for our unit tests (ex: adhoc csv file)\n",
    "* A common DLT pipeline logic is used for both the prod and the test pipeline (the **yellow** in the graph)\n",
    "* An additional notebook containing all the unit tests is used in the TEST pipeline (the **blue `TEST_xxx` tables** in the image on the right side)\n",
    "\n",
    "\n",
    "<div><img width=\"1100\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-advanecd/DLT-advanced-unit-test-0.png\"/></div>\n",
    "\n",
    "## Accessing the DLT pipeline\n",
    "\n",
    "Your pipeline has been created! You can directly access the <a dbdemos-pipeline-id=\"dlt-test\" href=\"#joblist/pipelines/31fac930-f846-41f5-85ac-3ec032abf9a1\">Delta Live Table Pipeline for unit-test demo</a>.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=3782931733495456&notebook=%2FDLT-pipeline-to-test&demo_name=dlt-unit-test&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-unit-test%2FDLT-pipeline-to-test&version=1&user_hash=f54348b201997908b91ace6288a9864114e7faea0de6a910579a7ab80989b7e0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a35678-f199-4c62-b5d7-c367c08d0d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Main Pipeline definition\n",
    "\n",
    "<img style=\"float: right\" width=\"700px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-advanecd/DLT-advanced-unit-test-2.png\"/>\n",
    "\n",
    "This notebook contains the main pipeline definition, the one we want to test (in yellow in the diagram).\n",
    "\n",
    "For this example, we centralized our main expectations in a metadata table that we'll use in the table definition.\n",
    "\n",
    "Theses expectations are your usual expectations, used to ensure and track data quality during the ingestion process. \n",
    "\n",
    "We can then build DBSQL dashboard on top of it and triggers alarms when we see error in our data (ex: incompatible schema, increasing our expectation count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69693ec7-37e2-4385-91ff-69e9832f5959",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define all our expectations as a metadata table"
    }
   },
   "outputs": [],
   "source": [
    "# In this example, we'll store our rules as a delta table for more flexibility & reusability. \n",
    "# While this isn't directly related to Unit test, it can also help for programatical analysis/reporting.\n",
    "catalog = \"dbdemos\"\n",
    "schema = dbName = db = \"dbdemos_dlt_unit_test\"\n",
    "\n",
    "data = [\n",
    " # tag/table name      name              constraint\n",
    " (\"user_bronze_dlt\",  \"correct_schema\", \"_rescued_data IS NULL\"),\n",
    " (\"user_silver_dlt\",  \"valid_id\",       \"id IS NOT NULL AND id > 0\"),\n",
    " (\"spend_silver_dlt\", \"valid_id\",       \"id IS NOT NULL AND id > 0\"),\n",
    " (\"user_gold_dlt\",    \"valid_age\",      \"age IS NOT NULL\"),\n",
    " (\"user_gold_dlt\",    \"valid_income\",   \"annual_income IS NOT NULL\"),\n",
    " (\"user_gold_dlt\",    \"valid_score\",    \"spending_core IS NOT NULL\")\n",
    "]\n",
    "#Typically only run once, this doesn't have to be part of the DLT pipeline.\n",
    "spark.createDataFrame(data=data, schema=[\"tag\", \"name\", \"constraint\"]).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc80e88a-55f9-490d-b1c7-946d59428845",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Make expectations portable and reusable from a Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "#Return the rules matching the tag as a format ready for DLT annotation.\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "def get_rules(tag):\n",
    "  \"\"\"\n",
    "    loads data quality rules from csv file\n",
    "    :param tag: tag to match\n",
    "    :return: dictionary of rules that matched the tag\n",
    "  \"\"\"\n",
    "  rules = {}\n",
    "  df = spark.table(f\"{catalog}.{schema}.expectations\").where(f\"tag = '{tag}'\")\n",
    "  for row in df.collect():\n",
    "    rules[row['name']] = row['constraint']\n",
    "  return rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b494ea-6744-4445-ac3b-73398be1b5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data\n",
    "\n",
    "This is the first step of the pipeline. Note that we consume the data from the `raw_user_data` view.\n",
    "\n",
    "This view is defined in the ingestion notebooks:\n",
    "* For PROD: [./ingestion_profile/DLT-ingest_prod]($./ingestion_profile/DLT-ingest_prod), reading from prod system (ex: kafka)\n",
    "* For TEST: [./ingestion_profile/DLT-ingest_test]($./ingestion_profile/DLT-ingest_test), consuming the test dataset (csv files)\n",
    "\n",
    "Start by reviewing the notebooks to see how the data is ingested.\n",
    "\n",
    "\n",
    "*Note: DLT is available as SQL or Python, this example will use Python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19fa9d0a-c1a8-4e36-9385-cf9d024d9a80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw User stream data in incremental mode"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table(comment=\"Raw user data\")\n",
    "@dlt.expect_all_or_drop(get_rules('user_bronze_dlt')) #get the rules from our centralized table.\n",
    "def user_bronze_dlt():\n",
    "  return dlt.read_stream(\"raw_user_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea422c5-cabf-4aa2-88b8-2f360c632752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2/ Customer Silver layer\n",
    "The silver layer is consuming **incremental** data from the bronze one, and cleaning up some information.\n",
    "\n",
    "We're also adding an expectation on the ID. As the ID will be used in the next join operation, ID should never be null and be positive.\n",
    "\n",
    "Note that the expectations have been defined in the metadata expectation table under `user_silver_dlt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1f811a-c160-4910-8675-613e068d6151",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and anonymize User data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(comment=\"User data cleaned and anonymized for analysis.\")\n",
    "@dlt.expect_all_or_drop(get_rules('user_silver_dlt'))\n",
    "def user_silver_dlt():\n",
    "  return (\n",
    "    dlt.read_stream(\"user_bronze_dlt\").select(\n",
    "      col(\"id\").cast(\"int\"),\n",
    "      sha1(\"email\").alias(\"email\"),\n",
    "      to_timestamp(col(\"creation_date\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"creation_date\"),\n",
    "      to_timestamp(col(\"last_activity_date\"),\"MM-dd-yyyy HH:mm:ss\").alias(\"last_activity_date\"),\n",
    "      \"firstname\", \n",
    "      \"lastname\", \n",
    "      \"address\", \n",
    "      \"city\", \n",
    "      \"last_ip\", \n",
    "      \"postcode\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d094afe-8fce-4592-843e-317aec403dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3/ Ingest spend information\n",
    "\n",
    "This is the same logic as for the customer data, we consume from the view defined in the TEST/PROD ingestion notebooks.\n",
    "\n",
    "We're also adding an expectation on the ID column as we'll join the 2 tables based on this field, and we want to track it's data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752369c8-d3f5-4563-a2da-5ecad0d2e7b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest user spending score"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"Spending score from raw data\")\n",
    "@dlt.expect_all_or_drop(get_rules('spend_silver_dlt'))\n",
    "def spend_silver_dlt():\n",
    "    return dlt.read_stream(\"raw_spend_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ed9a298-e030-4785-890e-e5ef175fb561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4/ Joining the 2 tables to create the gold layer\n",
    "We can now join the 2 tables on customer ID to create our final gold table.\n",
    "\n",
    "As our ML model will be using `age`, `annual_income` and `spending_score` we're adding expectation to only keep valid entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39f049b3-347e-450d-bc23-4651ece3ccb7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join both data to create our final table"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"Final user table with all information for Analysis / ML\")\n",
    "@dlt.expect_all_or_drop(get_rules('user_gold_dlt'))\n",
    "def user_gold_dlt():\n",
    "  return dlt.read_stream(\"user_silver_dlt\").join(dlt.read(\"spend_silver_dlt\"), [\"id\"], \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5827de49-dec3-425d-8005-500c9a9a17b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Our pipeline is now ready to be tested!\n",
    "\n",
    "Our pipeline now entirely defined.\n",
    "\n",
    "Here are a couple of example we might want to test:\n",
    "\n",
    "* Are we safely handling wrong data type as entry (ex: customer ID is sent as an incompatible STRING)\n",
    "* Are we resilient to NULL values in our primary keys\n",
    "* Are we enforcing uniqueness in our primary keys\n",
    "* Are we properly applying business logic (ex: proper aggregation, anonymization of PII field etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748e5649-9f67-4fbe-8fc0-f3b07fe88eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating the test dataset\n",
    "\n",
    "The next step is to create a test dataset.\n",
    "\n",
    "Creating the test dataset is a critical step. As any Unit tests, we need to add all possible data variation to ensure our logic is properly implemented.\n",
    "\n",
    "As example, let's make sure we'll ingest data having NULL id or ids as string.\n",
    "\n",
    "Open the [./test/DLT-Test-Dataset-setup]($./test/DLT-Test-Dataset-setup) notebook to see how this is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5edb266-1c8e-4c0c-ac5e-6a8262d9780c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining the Unit Tests\n",
    "\n",
    "We now have the data ready.\n",
    "\n",
    "The final step is creating the actual test.\n",
    "\n",
    "Open the [./test/DLT-Tests]($./test/DLT-Tests) notebook to see how this is done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "108f1557-2f05-46de-9b96-ee277c42a38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# That's it! our pipeline is fully ready & tested.\n",
    "\n",
    "We can then process as usual: build dashboard to track production metrics (ex: data quality & quantity) but also BI reporting & Data Science for final business use-case leveraging the Lakehouse:\n",
    "\n",
    "Here is a full example of the test pipeline definition.\n",
    "\n",
    "Note that we have 3 notebooks in the DLT pipeline:\n",
    "\n",
    "* **DLT-ingest_test**: ingesting our test datasets\n",
    "* **DLT-pipeline-to-test**: the actual pipeline we want to test\n",
    "* **test/DLT-Tests**: the test definition\n",
    "\n",
    "Remember that you'll have to schedule FULL REFRESH everytime your run the pipeline to get accurate test results (we want to consume all the entry dataset from scratch).\n",
    "\n",
    "This test pipeline can be scheduled to run within a Workflow, or as part of a CICD step (ex: triggered after a git commit)\n",
    "\n",
    "```\n",
    "{\n",
    "    \"clusters\": [\n",
    "        {\n",
    "            \"label\": \"default\",\n",
    "            \"autoscale\": {\n",
    "                \"min_workers\": 1,\n",
    "                \"max_workers\": 5\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"development\": true,\n",
    "    \"continuous\": false,\n",
    "    \"channel\": \"CURRENT\",\n",
    "    \"edition\": \"advanced\",\n",
    "    \"libraries\": [\n",
    "        {\n",
    "            \"notebook\": {\n",
    "                \"path\": \"/Repos/xxxx/Delta-Live-Table-Unit-Test/ingestion_profile/DLT-ingest_test\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"notebook\": {\n",
    "                \"path\": \"/Repos/xxxx/Delta-Live-Table-Unit-Test/DLT-pipeline-to-test\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"notebook\": {\n",
    "                \"path\": \"/Repos/xxxx/Delta-Live-Table-Unit-Test/test/DLT-Tests\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"name\": \"dbdemos_dlt_unit_test_{{CATALOG}}_{{SCHEMA}}\",\n",
    "    \"catalog\": \"{{CATALOG}}\",\n",
    "    \"target\": \"{{SCHEMA}}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a559271-f0e3-4343-9a1d-6b56550715cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Going further with DLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dddcdda8-98d8-4294-8d74-557f7a5e12a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checking your data quality metrics with Delta Live Table\n",
    "Delta Live Tables tracks all your data quality metrics. You can leverage the expecations directly as SQL table with Databricks SQL to track your expectation metrics and send alerts as required. This let you build the following dashboards:\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/retail-dlt-data-quality-dashboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edf4418f-fce1-4d18-8871-5d1853f66e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building our first business dashboard with Databricks SQL\n",
    "\n",
    "Once the data is ingested, we switch to Databricks SQL to build a new dashboard based on all the data we ingested.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/retail-dashboard.png\"/>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DLT-pipeline-to-test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
