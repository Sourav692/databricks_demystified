{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37505666-fc9a-4230-bb3c-37032df8d46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simple ETL with DLT\n",
    "\n",
    "DLT makes Data Engineering accessible for all. Just declare your transformations in SQL or Python, and DLT will handle the Data Engineering complexity for you.\n",
    "\n",
    "<img style=\"float:right\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-loan-1.png\" width=\"700\"/>\n",
    "\n",
    "**Accelerate ETL development** <br/>\n",
    "Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance\n",
    "\n",
    "**Remove operational complexity** <br/>\n",
    "By automating complex administrative tasks and gaining broader visibility into pipeline operations\n",
    "\n",
    "**Trust your data** <br/>\n",
    "With built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML\n",
    "\n",
    "**Simplify batch and streaming** <br/>\n",
    "With self-optimization and auto-scaling data pipelines for batch or streaming processing\n",
    "\n",
    "## Simple ingestion with Lakeflow Connect\n",
    "\n",
    "Lakeflow Connect offers built-in data ingestion connectors for popular SaaS applications, databases and file sources, such as Salesforce, Workday, and SQL Server to build incremental data pipelines at scale, fully integrated with Databricks.\n",
    "\n",
    "To give it a try, check our [Lakeflow Connect Product Tour](https://www.databricks.com/resources/demos/tours/platform/discover-databricks-lakeflow-connect-demo)\n",
    "\n",
    "## Our DLT pipeline\n",
    "\n",
    "We'll be using as input a raw dataset containing information on our customers Loan and historical transactions.\n",
    "\n",
    "Our goal is to ingest this data in near real time and build table for our Analyst team while ensuring data quality.\n",
    "\n",
    "**Your DLT Pipeline is ready!** Your pipeline was started using the SQL notebook and is <a dbdemos-pipeline-id=\"dlt-loans\" href=\"/#joblist/pipelines/18fe174b-523d-456f-9f6d-eba868434c66\">available here</a>.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=3782931733495456&notebook=%2F02-DLT-Loan-pipeline-PYTHON&demo_name=dlt-loans&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-loans%2F02-DLT-Loan-pipeline-PYTHON&version=1&user_hash=f54348b201997908b91ace6288a9864114e7faea0de6a910579a7ab80989b7e0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af73d23e-e235-4cca-9b2a-67a86880a690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Our datasets are coming from 3 different systems and saved under a cloud storage folder (S3/ADLS/GCS):\n",
    "\n",
    "* `loans/raw_transactions` (loans uploader here in every few minutes)\n",
    "* `loans/ref_accounting_treatment` (reference table, mostly static)\n",
    "* `loans/historical_loan` (loan from legacy system, new data added every week)\n",
    "\n",
    "Let's ingest this data incrementally, and then compute a couple of aggregates that we'll need for our final Dashboard to report our KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ebb42e-cbb7-4c86-b697-2ba59808edc7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's review the incoming data"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to explore the raw data\n",
    "dbutils.fs.ls('/Volumes/dbdemos/dbdemos_dlt_loan/raw_data/raw_transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9990e03-d2e7-49c6-a71c-a6624cffa4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Bronze layer: incrementally ingest data leveraging Databricks Autoloader\n",
    "\n",
    "<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-loan-2.png\" width=\"600\"/>\n",
    "\n",
    "Our raw data is being sent to a blob storage.\n",
    "\n",
    "Autoloader simplify this ingestion, including schema inference, schema evolution while being able to scale to millions of incoming files.\n",
    "\n",
    "Autoloader is available in Python using the `cloud_files` format and can be used with a variety of format (json, csv, avro...):\n",
    "\n",
    "\n",
    "#### STREAMING LIVE TABLE\n",
    "Defining tables as `STREAMING` will guarantee that you only consume new incoming data. Without `STREAMING`, you will scan and ingest all the data available at once. See the [documentation](https://docs.databricks.com/aws/en/dlt/flows) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b5150b-2755-4ba8-ac38-6efee99c0f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"New raw loan data incrementally ingested from cloud object storage landing zone\"\n",
    ")\n",
    "def raw_txs():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(\"/Volumes/dbdemos/dbdemos_dlt_loan/raw_data/raw_transactions\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26922e50-96e7-4518-bb6c-8f5e0628c108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(comment=\"Lookup mapping for accounting codes\")\n",
    "def ref_accounting_treatment():\n",
    "    return spark.read.format(\"delta\").load(\n",
    "        \"/Volumes/dbdemos/dbdemos_dlt_loan/raw_data/ref_accounting_treatment\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e432e439-f343-493a-ad3f-d777cc7807ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(comment=\"Raw historical transactions\")\n",
    "def raw_historical_loans():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(\"/Volumes/dbdemos/dbdemos_dlt_loan/raw_data/historical_loans\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddcac696-47c7-4a29-829f-f4eead2cbca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Silver layer: joining tables while ensuring data quality\n",
    "\n",
    "<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-loan-3.png\" width=\"600\"/>\n",
    "\n",
    "Once the bronze layer is defined, we'll create the sliver layers by joining data.\n",
    "\n",
    "To consume only incremental data from the Bronze layer like `BZ_raw_txs`, we'll be using the `spark.readStream.table` function.\n",
    "\n",
    "Note that we don't have to worry about compactions, DLT handles that for us.\n",
    "\n",
    "#### Expectations\n",
    "By defining expectations (`@dlt.expect`), you can enforce and track your data quality. See the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21cccd2d-17cf-449c-9aa5-e9884da75fa7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "enrich transactions with metadata"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.view(comment=\"Livestream of new transactions\")\n",
    "def new_txs():\n",
    "    txs = spark.readStream.table(\"raw_txs\").alias(\"txs\")\n",
    "    ref = spark.read.table(\"ref_accounting_treatment\").alias(\"ref\")\n",
    "    return txs.join(\n",
    "        ref, F.col(\"txs.accounting_treatment_id\") == F.col(\"ref.id\"), \"inner\"\n",
    "    ).selectExpr(\"txs.*\", \"ref.accounting_treatment as accounting_treatment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74d8008-26f8-4922-a3db-ec82e259da24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Keep only the proper transactions. Fail if cost center isn't correct, discard the others."
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(comment=\"Livestream of new transactions, cleaned and compliant\")\n",
    "@dlt.expect(\"Payments should be this year\", \"(next_payment_date > date('2020-12-31'))\")\n",
    "@dlt.expect_or_drop(\n",
    "    \"Balance should be positive\", \"(balance > 0 AND arrears_balance > 0)\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"Cost center must be specified\", \"(cost_center_code IS NOT NULL)\")\n",
    "def cleaned_new_txs():\n",
    "    return spark.readStream.table(\"new_txs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b090ca28-71b3-46e8-b03c-4e42a8acf450",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's quarantine the bad transaction for further analysis"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is the inverse condition of the above statement to quarantine incorrect data for further analysis.\n",
    "@dlt.table(comment=\"Incorrect transactions requiring human analysis\")\n",
    "@dlt.expect(\"Payments should be this year\", \"(next_payment_date <= date('2020-12-31'))\")\n",
    "@dlt.expect_or_drop(\n",
    "    \"Balance should be positive\", \"(balance <= 0 OR arrears_balance <= 0)\"\n",
    ")\n",
    "def quarantine_bad_txs():\n",
    "    return spark.readStream.table(\"new_txs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef7389b-dfc7-426b-973d-9ae81aea25fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enrich all historical transactions"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(comment=\"Historical loan transactions\")\n",
    "@dlt.expect(\"Grade should be valid\", \"(grade in ('A', 'B', 'C', 'D', 'E', 'F', 'G'))\")\n",
    "@dlt.expect_or_drop(\"Recoveries shoud be int\", \"(CAST(recoveries as INT) IS NOT NULL)\")\n",
    "def historical_txs():\n",
    "    history = spark.readStream.table(\"raw_historical_loans\").alias(\"l\")\n",
    "    ref = spark.read.table(\"ref_accounting_treatment\").alias(\"ref\")\n",
    "    return history.join(\n",
    "        ref, F.col(\"l.accounting_treatment_id\") == F.col(\"ref.id\"), \"inner\"\n",
    "    ).selectExpr(\"l.*\", \"ref.accounting_treatment as accounting_treatment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9beeae21-b810-47e9-a041-1406409a319e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Gold layer\n",
    "\n",
    "<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-loan-4.png\" width=\"600\"/>\n",
    "\n",
    "Our last step is to materialize the Gold Layer.\n",
    "\n",
    "Because these tables will be requested at scale using a SQL Endpoint, we'll add Liquid Clustering at the table level to organize data for faster queries, and DLT will handle the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a441bc-dbe3-49ea-9314-4f6e1f1b4dbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Balance aggregate per cost location"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    comment=\"Combines historical and new loan data for unified rollup of loan balances\",\n",
    "    cluster_by=[\"location_code\"],\n",
    ")\n",
    "def total_loan_balances():\n",
    "    return (\n",
    "        spark.read.table(\"historical_txs\")\n",
    "        .groupBy(\"addr_state\")\n",
    "        .agg(F.sum(\"revol_bal\").alias(\"bal\"))\n",
    "        .withColumnRenamed(\"addr_state\", \"location_code\")\n",
    "        .union(\n",
    "            spark.read.table(\"cleaned_new_txs\")\n",
    "            .groupBy(\"country_code\")\n",
    "            .agg(F.sum(\"balance\").alias(\"bal\"))\n",
    "            .withColumnRenamed(\"country_code\", \"location_code\")\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a0857e-048c-4589-9abf-39b1f2da4124",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Balance aggregate per cost center"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    comment=\"Live table of new loan balances for consumption by different cost centers\"\n",
    ")\n",
    "def new_loan_balances_by_cost_center():\n",
    "    return (\n",
    "        spark.read.table(\"cleaned_new_txs\")\n",
    "        .groupBy(\"cost_center_code\")\n",
    "        .agg(F.sum(\"balance\").alias(\"sum_balance\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69809a8c-9b96-43b4-8d23-0d8cd64992fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Balance aggregate per country"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(comment=\"Live table of new loan balances per country\")\n",
    "def new_loan_balances_by_country():\n",
    "    return (\n",
    "        spark.read.table(\"cleaned_new_txs\")\n",
    "        .groupBy(\"country_code\")\n",
    "        .agg(F.sum(\"count\").alias(\"sum_count\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "742f504c-f1b5-4ecb-a7af-9546fff0bc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Your DLT pipeline is ready to be started. <a dbdemos-pipeline-id=\"dlt-loans\" href=\"/#joblist/pipelines/18fe174b-523d-456f-9f6d-eba868434c66\">Click here to access the pipeline</a> created for you using the SQL notebook.\n",
    "\n",
    "To create a new one using this notebook, open the DLT menu, create a pipeline and select this notebook to run it. To generate sample data, please run the [companion notebook]($./_resources/00-Loan-Data-Generator) (make sure the path where you read and write the data are the same!)\n",
    "\n",
    "Datas Analyst can start using DBSQL to analyze data and track our Loan metrics.  Data Scientist can also access the data to start building models to predict payment default or other more advanced use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d2704d-4038-4746-b15f-65220da4ac8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tracking data quality\n",
    "\n",
    "Expectations stats are automatically available as system table.\n",
    "\n",
    "This information let you monitor your data ingestion quality.\n",
    "\n",
    "You can leverage DBSQL to request these table and build custom alerts based on the metrics your business is tracking.\n",
    "\n",
    "\n",
    "See [how to access your DLT metrics]($./03-Log-Analysis)\n",
    "\n",
    "<img width=\"500\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/dlt/dlt-loans-dashboard.png?raw=true\">\n",
    "\n",
    "<a dbdemos-dashboard-id=\"dlt-expectations\" href='/sql/dashboardsv3/01f03f2ee4bb1cb9be3ce8b69670e763' target=\"_blank\">Data Quality Dashboard example</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-DLT-Loan-pipeline-PYTHON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
