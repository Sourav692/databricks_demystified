{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51851d1c-887b-47c1-8ec4-c0ec47f14988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa17c87-09f3-4c4c-b719-2f93db621425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Retail CDC Data Generator\n",
    "\n",
    "Run this notebook to create new data. It's added in the pipeline to make sure data exists when we run it.\n",
    "\n",
    "You can also run it in the background to periodically add data.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=3782931733495456&notebook=%2F_resources%2F00-Data_CDC_Generator&demo_name=dlt-cdc&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-cdc%2F_resources%2F00-Data_CDC_Generator&version=1&user_hash=f54348b201997908b91ace6288a9864114e7faea0de6a910579a7ab80989b7e0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7396c5-c75f-4f8c-82bd-e275edcd902c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"dbdemos\"\n",
    "schema = dbName = db = \"dbdemos_dlt_cdc\"\n",
    "\n",
    "volume_name = \"raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5da371-8795-4422-90ba-22d2c90f0672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE CATALOG IF NOT EXISTS `{catalog}`')\n",
    "spark.sql(f'USE CATALOG `{catalog}`')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`')\n",
    "spark.sql(f'USE SCHEMA `{schema}`')\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume_name}`')\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/{volume_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca30062-9cd3-4f90-bdc1-2082a93131c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  dbutils.fs.ls(volume_folder+\"/transactions\")\n",
    "  dbutils.fs.ls(volume_folder+\"/customers\")\n",
    "except:  \n",
    "  print(f\"folder doesn't exists, generating the data under {volume_folder}...\")\n",
    "  from pyspark.sql import functions as F\n",
    "  from faker import Faker\n",
    "  from collections import OrderedDict \n",
    "  import uuid\n",
    "  fake = Faker()\n",
    "  import random\n",
    "\n",
    "\n",
    "  fake_firstname = F.udf(fake.first_name)\n",
    "  fake_lastname = F.udf(fake.last_name)\n",
    "  fake_email = F.udf(fake.ascii_company_email)\n",
    "  fake_date = F.udf(lambda:fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "  fake_address = F.udf(fake.address)\n",
    "  operations = OrderedDict([(\"APPEND\", 0.5),(\"DELETE\", 0.1),(\"UPDATE\", 0.3),(None, 0.01)])\n",
    "  fake_operation = F.udf(lambda:fake.random_elements(elements=operations, length=1)[0])\n",
    "  fake_id = F.udf(lambda: str(uuid.uuid4()) if random.uniform(0, 1) < 0.98 else None)\n",
    "\n",
    "  df = spark.range(0, 100000).repartition(100)\n",
    "  df = df.withColumn(\"id\", fake_id())\n",
    "  df = df.withColumn(\"firstname\", fake_firstname())\n",
    "  df = df.withColumn(\"lastname\", fake_lastname())\n",
    "  df = df.withColumn(\"email\", fake_email())\n",
    "  df = df.withColumn(\"address\", fake_address())\n",
    "  df = df.withColumn(\"operation\", fake_operation())\n",
    "  df_customers = df.withColumn(\"operation_date\", fake_date())\n",
    "  df_customers.repartition(100).write.format(\"json\").mode(\"overwrite\").save(volume_folder+\"/customers\")\n",
    "\n",
    "  df = spark.range(0, 10000).repartition(20)\n",
    "  df = df.withColumn(\"id\", fake_id())\n",
    "  df = df.withColumn(\"transaction_date\", fake_date())\n",
    "  df = df.withColumn(\"amount\", F.round(F.rand()*1000))\n",
    "  df = df.withColumn(\"item_count\", F.round(F.rand()*10))\n",
    "  df = df.withColumn(\"operation\", fake_operation())\n",
    "  df = df.withColumn(\"operation_date\", fake_date())\n",
    "  #Join with the customer to get the same IDs generated.\n",
    "  df = df.withColumn(\"t_id\", F.monotonically_increasing_id()).join(spark.read.json(volume_folder+\"/customers\").select(\"id\").withColumnRenamed(\"id\", \"customer_id\").withColumn(\"t_id\", F.monotonically_increasing_id()), \"t_id\").drop(\"t_id\")\n",
    "  df.repartition(10).write.format(\"json\").mode(\"overwrite\").save(volume_folder+\"/transactions\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-Data_CDC_Generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
